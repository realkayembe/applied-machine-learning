{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ca894ff3",
      "metadata": {
        "id": "ca894ff3"
      },
      "source": [
        "# Twitter Sentiment Analysis on Datasets for Low Resources Languages - Nigerian Languages Case Study with Pre-trained Models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the train and test dataset located at and place them in the same directory as this Notebook:\n",
        "https://github.com/hausanlp/NaijaSenti/tree/main/data/annotated_tweets\n",
        "\n",
        "For example, for Yoruba language:\n",
        "  Train dataset:https://github.com/hausanlp/NaijaSenti/blob/main/data/annotated_tweets/yor/train.tsv\n",
        "  Test dataset: https://github.com/hausanlp/NaijaSenti/blob/main/data/annotated_tweets/yor/test.tsv"
      ],
      "metadata": {
        "id": "EJ5nz5pzykIJ"
      },
      "id": "EJ5nz5pzykIJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7d0256d",
      "metadata": {
        "id": "a7d0256d"
      },
      "outputs": [],
      "source": [
        "!pip install accelerate -U\n",
        "!pip install transformers torch sklearn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfe8442e",
      "metadata": {
        "id": "dfe8442e"
      },
      "source": [
        "## Setup and Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "dc764ad4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc764ad4",
        "outputId": "6626329c-15ad-4383-b4aa-6cf1b6999295"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "import pandas as pd\n",
        "\n",
        "def load_data(file_path):\n",
        "    return pd.read_csv(file_path, delimiter='\\t')\n",
        "\n",
        "train_df = load_data('train.tsv')\n",
        "test_df = load_data('test.tsv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca1f517b",
      "metadata": {
        "id": "ca1f517b"
      },
      "source": [
        "## Model: camembert-base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49fe465c",
      "metadata": {
        "id": "49fe465c"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Tokenization and Model Setup for camembert-base\n",
        "tokenizer = AutoTokenizer.from_pretrained('camembert-base')\n",
        "model = AutoModelForSequenceClassification.from_pretrained('camembert-base', num_labels=2)\n",
        "\n",
        "# Prepare dataset\n",
        "train_encodings = tokenizer(train_df['tweet'].tolist(), truncation=True, padding=True, max_length=128)\n",
        "test_encodings = tokenizer(test_df['tweet'].tolist(), truncation=True, padding=True, max_length=128)\n",
        "\n",
        "train_dataset = torch.utils.data.TensorDataset(torch.tensor(train_encodings['input_ids']), torch.tensor(train_df['label'].apply(lambda x: 1 if x == 'positive' else 0).tolist()))\n",
        "test_dataset = torch.utils.data.TensorDataset(torch.tensor(test_encodings['input_ids']), torch.tensor(test_df['label'].apply(lambda x: 1 if x == 'positive' else 0).tolist()))\n",
        "\n",
        "# Training Arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results_camembert-base',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs_camembert-base',\n",
        ")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset\n",
        ")\n",
        "\n",
        "# Train and Evaluate\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "096ea646",
      "metadata": {
        "id": "096ea646"
      },
      "source": [
        "## Model: bert-base-cased"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9ea5ec6",
      "metadata": {
        "id": "f9ea5ec6"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Tokenization and Model Setup for bert-base-cased\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
        "model = AutoModelForSequenceClassification.from_pretrained('bert-base-cased', num_labels=2)\n",
        "\n",
        "# Prepare dataset\n",
        "train_encodings = tokenizer(train_df['tweet'].tolist(), truncation=True, padding=True, max_length=128)\n",
        "test_encodings = tokenizer(test_df['tweet'].tolist(), truncation=True, padding=True, max_length=128)\n",
        "\n",
        "train_dataset = torch.utils.data.TensorDataset(torch.tensor(train_encodings['input_ids']), torch.tensor(train_df['label'].apply(lambda x: 1 if x == 'positive' else 0).tolist()))\n",
        "test_dataset = torch.utils.data.TensorDataset(torch.tensor(test_encodings['input_ids']), torch.tensor(test_df['label'].apply(lambda x: 1 if x == 'positive' else 0).tolist()))\n",
        "\n",
        "# Training Arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results_bert-base-cased',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs_bert-base-cased',\n",
        ")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset\n",
        ")\n",
        "\n",
        "# Train and Evaluate\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "165ebf75",
      "metadata": {
        "id": "165ebf75"
      },
      "source": [
        "## Model: xlnet-base-cased"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc28ccab",
      "metadata": {
        "id": "bc28ccab"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Tokenization and Model Setup for xlnet-base-cased\n",
        "tokenizer = AutoTokenizer.from_pretrained('xlnet-base-cased')\n",
        "model = AutoModelForSequenceClassification.from_pretrained('xlnet-base-cased', num_labels=2)\n",
        "\n",
        "# Prepare dataset\n",
        "train_encodings = tokenizer(train_df['tweet'].tolist(), truncation=True, padding=True, max_length=128)\n",
        "test_encodings = tokenizer(test_df['tweet'].tolist(), truncation=True, padding=True, max_length=128)\n",
        "\n",
        "train_dataset = torch.utils.data.TensorDataset(torch.tensor(train_encodings['input_ids']), torch.tensor(train_df['label'].apply(lambda x: 1 if x == 'positive' else 0).tolist()))\n",
        "test_dataset = torch.utils.data.TensorDataset(torch.tensor(test_encodings['input_ids']), torch.tensor(test_df['label'].apply(lambda x: 1 if x == 'positive' else 0).tolist()))\n",
        "\n",
        "# Training Arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results_xlnet-base-cased',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs_xlnet-base-cased',\n",
        ")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset\n",
        ")\n",
        "\n",
        "# Train and Evaluate\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38ef2bd0",
      "metadata": {
        "id": "38ef2bd0"
      },
      "source": [
        "## Model: albert-base-v2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8034d034",
      "metadata": {
        "id": "8034d034"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Tokenization and Model Setup for albert-base-v2\n",
        "tokenizer = AutoTokenizer.from_pretrained('albert-base-v2')\n",
        "model = AutoModelForSequenceClassification.from_pretrained('albert-base-v2', num_labels=2)\n",
        "\n",
        "# Prepare dataset\n",
        "train_encodings = tokenizer(train_df['tweet'].tolist(), truncation=True, padding=True, max_length=128)\n",
        "test_encodings = tokenizer(test_df['tweet'].tolist(), truncation=True, padding=True, max_length=128)\n",
        "\n",
        "train_dataset = torch.utils.data.TensorDataset(torch.tensor(train_encodings['input_ids']), torch.tensor(train_df['label'].apply(lambda x: 1 if x == 'positive' else 0).tolist()))\n",
        "test_dataset = torch.utils.data.TensorDataset(torch.tensor(test_encodings['input_ids']), torch.tensor(test_df['label'].apply(lambda x: 1 if x == 'positive' else 0).tolist()))\n",
        "\n",
        "# Training Arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results_albert-base-v2',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs_albert-base-v2',\n",
        ")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset\n",
        ")\n",
        "\n",
        "# Train and Evaluate\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00591e7e",
      "metadata": {
        "id": "00591e7e"
      },
      "source": [
        "## Model: distilroberta-base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3cfcdb3",
      "metadata": {
        "id": "c3cfcdb3"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Tokenization and Model Setup for distilroberta-base\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilroberta-base')\n",
        "model = AutoModelForSequenceClassification.from_pretrained('distilroberta-base', num_labels=2)\n",
        "\n",
        "# Prepare dataset\n",
        "train_encodings = tokenizer(train_df['tweet'].tolist(), truncation=True, padding=True, max_length=128)\n",
        "test_encodings = tokenizer(test_df['tweet'].tolist(), truncation=True, padding=True, max_length=128)\n",
        "\n",
        "train_dataset = torch.utils.data.TensorDataset(torch.tensor(train_encodings['input_ids']), torch.tensor(train_df['label'].apply(lambda x: 1 if x == 'positive' else 0).tolist()))\n",
        "test_dataset = torch.utils.data.TensorDataset(torch.tensor(test_encodings['input_ids']), torch.tensor(test_df['label'].apply(lambda x: 1 if x == 'positive' else 0).tolist()))\n",
        "\n",
        "# Training Arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results_distilroberta-base',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs_distilroberta-base',\n",
        ")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset\n",
        ")\n",
        "\n",
        "# Train and Evaluate\n",
        "trainer.train()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}